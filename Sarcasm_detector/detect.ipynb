{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "      <td>huffingtonpost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "      <td>huffingtonpost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "      <td>theonion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "      <td>theonion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>huffingtonpost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \\\n",
       "0  former versace store clerk sues over secret 'b...             0   \n",
       "1  the 'roseanne' revival catches up to our thorn...             0   \n",
       "2  mom starting to fear son's web series closest ...             1   \n",
       "3  boehner just wants wife to listen, not come up...             1   \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0   \n",
       "\n",
       "           source  \n",
       "0  huffingtonpost  \n",
       "1  huffingtonpost  \n",
       "2        theonion  \n",
       "3        theonion  \n",
       "4  huffingtonpost  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sar_acc = pd.read_json('Sarcasm_Headlines_Dataset.json',lines=True)\n",
    "sar_acc['source'] = sar_acc['article_link'].apply(lambda x: re.findall(r'\\w+', x)[2])\n",
    "sar_acc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "sar_det = sar_acc[sar_acc.is_sarcastic==1]\n",
    "sar_det.reset_index(drop=True, inplace=True)\n",
    "acc_det = sar_acc[sar_acc.is_sarcastic==0]\n",
    "acc_det.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Tokenizing the Headlines of Sarcasm\n",
    "sar_news = []\n",
    "for rows in range(0, sar_det.shape[0]):\n",
    "    head_txt = sar_det.headline[rows]\n",
    "    head_txt = head_txt.split(\" \")\n",
    "    sar_news.append(head_txt)\n",
    "\n",
    "#Converting into single list for Sarcasm\n",
    "import itertools\n",
    "sar_list = list(itertools.chain(*sar_news))\n",
    "\n",
    "# Tokenizing the Headlines of Acclaim\n",
    "acc_news = []\n",
    "for rows in range(0, acc_det.shape[0]):\n",
    "    head_txt = acc_det.headline[rows]\n",
    "    head_txt = head_txt.split(\" \")\n",
    "    acc_news.append(head_txt)\n",
    "    \n",
    "#Converting into single list for Acclaim\n",
    "acc_list = list(itertools.chain(*acc_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package stopwords to /home/drex/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = list(stopwords.words(\"english\"))\n",
    "\n",
    "sar_list_restp = [word for word in sar_list if word.lower() not in stopwords]\n",
    "acc_list_restp = [word for word in acc_list if word.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common sarcastic words : [('man', 1021), ('new', 821), ('area', 477), ('report:', 360), ('woman', 290), ('one', 253), ('time', 213), ('still', 208), ('day', 207), ('trump', 200)]\n",
      "most common acclaimed words : [('trump', 957), ('new', 664), ('donald', 453), (\"trump's\", 364), ('says', 346), ('women', 240), ('one', 234), ('u.s.', 223), ('first', 220), ('make', 209)]\n"
     ]
    }
   ],
   "source": [
    "sar_count, acc_count = Counter(sar_list_restp), Counter(acc_list_restp)\n",
    "most_sar, most_acc = sar_count.most_common(10), acc_count.most_common(10)\n",
    "print(\"most common sarcastic words : \" + str(most_sar))\n",
    "print(\"most common acclaimed words : \" + str(most_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sar_acc.headline\n",
    "Y = sar_acc.is_sarcastic\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = tf.keras.preprocessing.text.Tokenizer(num_words=1000)\n",
    "tk.fit_on_texts(X_train)\n",
    "seqs = tk.texts_to_sequences(X_train)\n",
    "max_len = 100\n",
    "seqs_mat = tf.keras.preprocessing.sequence.pad_sequences(seqs,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_def():\n",
    "    inputs = tf.keras.layers.Input(name='inputs', shape=[max_len])\n",
    "    layer = tf.keras.layers.Embedding(1000,50,input_length=max_len)(inputs)\n",
    "    layer = tf.keras.layers.LSTM(64)(layer)\n",
    "    layer = tf.keras.layers.Dense(256,name='FC1')(layer)\n",
    "    layer = tf.keras.layers.Activation('relu')(layer)\n",
    "    layer = tf.keras.layers.Dropout(0.2)(layer)\n",
    "    layer = tf.keras.layers.Dense(1,name='out_layer')(layer)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 50)           50000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 96,337\n",
      "Trainable params: 96,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_def()\n",
    "model.summary()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19230 samples, validate on 2137 samples\n",
      "WARNING:tensorflow:From /home/drex/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/5\n",
      "19230/19230 [==============================] - 63s 3ms/sample - loss: 0.6632 - acc: 0.7308 - val_loss: 0.6416 - val_acc: 0.6022\n",
      "Epoch 2/5\n",
      "19230/19230 [==============================] - 53s 3ms/sample - loss: 6.8487 - acc: 0.5005 - val_loss: 8.5456 - val_acc: 0.4427\n",
      "Epoch 3/5\n",
      "19230/19230 [==============================] - 54s 3ms/sample - loss: 8.5716 - acc: 0.4410 - val_loss: 8.5456 - val_acc: 0.4427\n",
      "Epoch 4/5\n",
      "19230/19230 [==============================] - 53s 3ms/sample - loss: 8.5716 - acc: 0.4410 - val_loss: 8.5456 - val_acc: 0.4427\n",
      "Epoch 5/5\n",
      "19230/19230 [==============================] - 53s 3ms/sample - loss: 8.5716 - acc: 0.4410 - val_loss: 8.5456 - val_acc: 0.4427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7d3d2e0a58>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_acc', \n",
    "    mode='max',\n",
    "    patience=6\n",
    ")\n",
    "# increase epochs and other steps for better training\n",
    "model.fit(seqs_mat,Y_train,batch_size=100,epochs=5,\n",
    "          validation_split=0.1,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sarcasm(user_seq):\n",
    "#     prediction\n",
    "    prob = model.predict(user_seq)\n",
    "    probability = np.mean(prob, axis=0)\n",
    "\n",
    "    if probability > 0.5:\n",
    "        return(\"Sarcastic\")\n",
    "    elif probability < 0.5:\n",
    "        return(\"Not Sarcastic\")\n",
    "    elif probability == 0.5:\n",
    "        return(\"Neutral\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_text_processing(user_text):\n",
    "    user_text = user_text.split()\n",
    "    user_text = [word.lower() for word in user_text if word not in stopwords]\n",
    "#     user_text = [lemm.lemmatize(word) for word in user_text]\n",
    "    user_text\n",
    "    user_seq = np.array(user_text)\n",
    "    user_seq = tk.texts_to_sequences(user_seq)\n",
    "    user_seq = tf.keras.preprocessing.sequence.pad_sequences(user_seq,maxlen=max_len)\n",
    "\n",
    "    return user_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_sequences_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-4f712c4affc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sequences_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_sequences_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "est_sequences = tok.texts_to_sequences(X_test)\n",
    "\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 'state population to double by 2040, babies to blame' is of 'Sarcastic' nature\n"
     ]
    }
   ],
   "source": [
    "user_text = 'state population to double by 2040, babies to blame'\n",
    "user_seq = user_text_processing(user_text)\n",
    "user_seq\n",
    "prediction = predict_sarcasm(user_seq)\n",
    "print(f\"Sentence '{user_text}' is of '{prediction}' nature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
