{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "# import openpyxl\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.127513e+18</td>\n",
       "      <td>RT @_SwarajIndia: देश के चुनाव को सुनियोजित तर...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "      <td>hasoc_2020_hi_4007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.123822e+18</td>\n",
       "      <td>RT @YadavsAniruddh: #श्रीनगर में एक #आतंकवादी ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_hi_1548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.123733e+18</td>\n",
       "      <td>RT @KapilMishra_IND: चाइना ने नेहरू जी की बहुत...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_hi_612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.127646e+18</td>\n",
       "      <td>RT @HarishK04131926: @AcharyaPramodk बोलने से ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_hi_2044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.127768e+18</td>\n",
       "      <td>RT @DhirajY09648978: #BJP_भगाओ_देश_बचाओ\\n स्वा...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "      <td>hasoc_2020_hi_4304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id                                               text task1  \\\n",
       "0  1.127513e+18  RT @_SwarajIndia: देश के चुनाव को सुनियोजित तर...   HOF   \n",
       "1  1.123822e+18  RT @YadavsAniruddh: #श्रीनगर में एक #आतंकवादी ...   NOT   \n",
       "2  1.123733e+18  RT @KapilMishra_IND: चाइना ने नेहरू जी की बहुत...   NOT   \n",
       "3  1.127646e+18  RT @HarishK04131926: @AcharyaPramodk बोलने से ...   NOT   \n",
       "4  1.127768e+18  RT @DhirajY09648978: #BJP_भगाओ_देश_बचाओ\\n स्वा...   HOF   \n",
       "\n",
       "  task2                  ID  \n",
       "0  OFFN  hasoc_2020_hi_4007  \n",
       "1  NONE  hasoc_2020_hi_1548  \n",
       "2  NONE   hasoc_2020_hi_612  \n",
       "3  NONE  hasoc_2020_hi_2044  \n",
       "4  HATE  hasoc_2020_hi_4304  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_excel(\"dataset/hindi.xlsx\", engine=\"openpyxl\")\n",
    "wb = load_workbook(\"dataset/hindi.xlsx\")\n",
    "sheet = wb.active\n",
    "col = csv.writer(open(\"tt.csv\", 'w', newline=\"\"))\n",
    "for r in sheet.rows:\n",
    "    col.writerow([cell.value for cell in r])\n",
    "  \n",
    "df = pd.DataFrame(pd.read_csv(\"tt.csv\"))\n",
    "df.to_csv(\"dataset/hindi.csv\", sep=\",\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @YadavsAniruddh: #श्रीनगर में एक #आतंकवादी अव पूर्व #आतंकी हो गया , #भाजपा में शामिल होते ही उसके सारे पाप धुल गए! अब वह #भाजपा के टिकट…'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drex/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>task2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @_SwarajIndia: देश के चुनाव को सुनियोजित तर...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @YadavsAniruddh: #श्रीनगर में एक #आतंकवादी ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KapilMishra_IND: चाइना ने नेहरू जी की बहुत...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @HarishK04131926: @AcharyaPramodk बोलने से ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @DhirajY09648978: #BJP_भगाओ_देश_बचाओ\\n स्वा...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  task2\n",
       "0  RT @_SwarajIndia: देश के चुनाव को सुनियोजित तर...    1.0\n",
       "1  RT @YadavsAniruddh: #श्रीनगर में एक #आतंकवादी ...    0.0\n",
       "2  RT @KapilMishra_IND: चाइना ने नेहरू जी की बहुत...    0.0\n",
       "3  RT @HarishK04131926: @AcharyaPramodk बोलने से ...    0.0\n",
       "4  RT @DhirajY09648978: #BJP_भगाओ_देश_बचाओ\\n स्वा...    1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['text', 'task2']\n",
    "df_2 = df[cols]\n",
    "# df_2.replace(to_replace=[\"HATE, OFFN\"], value=\"1\")\n",
    "df_2['task2'] = df_2['task2'].map({'HATE':1,'OFFN':1, 'NONE':0})\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'उसका', 'वग़ैरह', 'सकते', 'हे', 'ना', 'पूरा', 'वाले', 'या', 'फिर', 'जिन्हों', 'पास', 'साथ', 'जिधर', 'वरग', 'करने', 'मैं', 'देकर', 'काफि', 'कोइ', 'मेरी', 'यही', 'भि', 'भी', 'कहते', 'कौन', 'मेरे', 'ऐसे', 'कोनसा', 'जा', 'उस', 'तब', 'जब', 'इसकी', 'दो', 'बाद', 'किर', 'सभी', 'पर', 'जीधर', 'होते', 'कि', 'यहि', 'वह', 'तुझे', 'निचे', 'द्वारा', 'वुह ', 'नहिं', 'कुछ', 'एसे', 'उनके', 'तू', 'मगर', 'ने', 'बनी', 'हि', 'ऱ्वासा', 'ओर', 'था', 'ले', 'बिलकुल', 'जिन्हें', 'आदि', 'हुइ', 'उंहों', 'तो', 'होता', 'इंहें', 'पहले', 'संग', 'किसि', 'यहां', 'वर्ग', 'इंहों', 'इन्हें', 'गया', 'यिह', 'उसे', 'दवारा', 'तिसे', 'अनुसार', 'जिसे', 'तुम', 'कहता', 'रवासा', 'एक', 'जिस', 'किसी', 'इसके', 'ये', 'जिंहों', 'एवं', 'वहाँ', 'के', 'अंदर', 'उन्हें', 'नीचे', 'तिन', 'होती', 'है', 'हुआ', 'तिंहें', \"इतयादि' ,'यहाँ\", 'अपनी', 'किन्हों', 'उसि', 'सकता', 'दुसरे', 'साबुत', 'उनका', 'उनकी', 'क्या', 'कौनसा', 'अर्थात', 'अभि', 'करना', 'कोन', 'निहायत', 'उन', 'किया', 'तरह', 'तिंहों', 'हुए', 'सो', 'करते', 'थे', 'कइ', 'पे', 'इस', 'इसकि', 'उनकि', 'और', 'बनि', 'जहां', 'उन्ह', 'रखें', 'इसि', 'उन्हों', 'किंहों', 'सभि', 'थी', 'हो', 'मे', 'तिन्हें', 'वगेरह', 'बही', 'रह', 'इन ', 'आप', 'सारा', 'काफ़ी', 'किन्हें', 'से', 'लिये', 'कर', 'अप', 'को', 'मुझे', 'हें', 'तिस', 'किसे', 'इसे', 'तेरा', 'उसको', 'जितना', 'व', 'दिया', 'करें', 'बहि', 'करता', 'परन्तु', 'तेरे', 'वहीं', 'अपने', 'उन्होंने', 'कितना', 'जैसे', 'लेकिन', 'अपनि', 'हैं', 'हुअ', 'एस', 'जिंहें', 'होने', 'जो', 'हम', 'इंहिं', 'वहां', 'यदि', 'उंहें', 'होना', 'दुसरा', 'इन्हीं', 'दूसरे', 'लिए', 'साभ', 'उनको', 'कुल', 'होति', 'उसी', 'दबारा', 'यिह ', 'हूं', 'यह', 'वह ', 'तेरी', 'तुम्हारे', 'रहे', 'अदि', 'इसका', 'किंहें', 'नहीं', 'की', 'सब', 'उसके', 'में', 'इन्हों', 'घर', 'इत्यादि', 'जिन', 'इन', \"बात' \", 'का', 'इसलिये', 'पुरा', 'पर  ', 'सबसे', 'दे', 'उसने', 'ऐसा', 'इसमें', 'उन्हीं', 'थि', 'भितर', 'कारण', 'जहाँ', 'अत', 'किस', 'बहुत', 'जैसा', 'वुह', 'कहा', 'कई', 'क्योंकि', 'जेसा', 'न', 'वे', 'कह', 'वहिं', 'हुई', 'साम्हने', 'तक', 'रहा', 'अभी', 'मानो', 'उसकी', 'मुझ', 'कोई', 'अपना', 'तिन्हों', 'इनका', 'इसी', 'बाला', 'ही', 'जेसे', 'भीतर', 'गए', 'उंहिं'}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "with open(\"dataset/Hindi_StopWords.txt\",encoding='utf-8') as f:\n",
    "    stopword= f.read().strip('\\ufeff')\n",
    "stopword = stopword.split(\", \")\n",
    "stopword = [i.strip(\"'\") for i in stopword]\n",
    "\n",
    "stopwords = set(stopword)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "hate_det = df_2[df_2.task2==1]\n",
    "hate_det.reset_index(drop=True, inplace=True)\n",
    "acc_det = df_2[df_2.task2==0]\n",
    "acc_det.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Tokenization\n",
    "hate_news = []\n",
    "for rows in range(0, hate_det.shape[0]):\n",
    "    head_txt = hate_det.text[rows]\n",
    "    head_txt = head_txt.split(\" \")\n",
    "    hate_news.append(head_txt)\n",
    "    \n",
    "hate_list = list(itertools.chain(*hate_news))\n",
    "# print(hate_list)\n",
    "acc_news = []\n",
    "for rows in range(0, acc_det.shape[0]):\n",
    "    head_txt = acc_det.text[rows]\n",
    "    head_txt = head_txt.split(\" \")\n",
    "    acc_news.append(head_txt)\n",
    "    \n",
    "acc_list = list(itertools.chain(*acc_news))\n",
    "# print(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_list_restp = [word for word in hate_list if word.lower() not in stopwords]\n",
    "acc_list_restp = [word for word in acc_list if word.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common hate words : [('RT', 169), ('', 106), ('मोदी', 49), ('देश', 36), ('नही', 33), ('वोट', 29), ('कांग्रेस', 24), ('जी', 23), ('वो', 21), ('अब', 19)]\n",
      "most common acclaimed words : [('RT', 535), ('', 291), ('जी', 100), ('हर', 91), ('नही', 90), ('मोदी', 79), ('वोट', 71), ('अब', 60), ('है।', 58), ('देश', 56)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "hate_count, acc_count = Counter(hate_list_restp), Counter(acc_list_restp)\n",
    "most_hate, most_acc = hate_count.most_common(10), acc_count.most_common(10)\n",
    "print(\"most common hate words : \" + str(most_hate))\n",
    "print(\"most common acclaimed words : \" + str(most_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_2.text\n",
    "Y = df_2.task2\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = tf.keras.preprocessing.text.Tokenizer(num_words=1000)\n",
    "tk.fit_on_texts(X_train)\n",
    "seqs = tk.texts_to_sequences(X_train)\n",
    "max_len = 100\n",
    "seqs_mat = tf.keras.preprocessing.sequence.pad_sequences(seqs,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_def():\n",
    "    inputs = tf.keras.layers.Input(name='inputs', shape=[max_len])\n",
    "    layer = tf.keras.layers.Embedding(1000,50,input_length=max_len)(inputs)\n",
    "    layer = tf.keras.layers.LSTM(64)(layer)\n",
    "    layer = tf.keras.layers.Dense(256,name='FC1')(layer)\n",
    "    layer = tf.keras.layers.Activation('relu')(layer)\n",
    "    layer = tf.keras.layers.Dropout(0.2)(layer)\n",
    "    layer = tf.keras.layers.Dense(1,name='out_layer')(layer)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 50)           50000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 96,337\n",
      "Trainable params: 96,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_def()\n",
    "model.summary()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 913 samples, validate on 102 samples\n",
      "Epoch 1/10\n",
      "913/913 [==============================] - 3s 3ms/sample - loss: -18.5773 - acc: 0.5465 - val_loss: -12.9058 - val_acc: 0.4118\n",
      "Epoch 2/10\n",
      "913/913 [==============================] - 3s 3ms/sample - loss: -16.9592 - acc: 0.3921 - val_loss: 8.5243 - val_acc: 0.3529\n",
      "Epoch 3/10\n",
      "913/913 [==============================] - 3s 3ms/sample - loss: -6.3355 - acc: 0.3998 - val_loss: -8.3964 - val_acc: 0.2549\n",
      "Epoch 4/10\n",
      "913/913 [==============================] - 3s 3ms/sample - loss: -14.3116 - acc: 0.2421 - val_loss: -8.1176 - val_acc: 0.2451\n",
      "Epoch 5/10\n",
      "913/913 [==============================] - 3s 3ms/sample - loss: -14.0905 - acc: 0.2366 - val_loss: -8.1176 - val_acc: 0.2451\n",
      "Epoch 6/10\n",
      "913/913 [==============================] - 3s 3ms/sample - loss: -14.0905 - acc: 0.2366 - val_loss: -8.1176 - val_acc: 0.2451\n",
      "Epoch 7/10\n",
      "913/913 [==============================] - 3s 3ms/sample - loss: -14.1227 - acc: 0.2377 - val_loss: -8.1176 - val_acc: 0.2451\n",
      "Epoch 8/10\n",
      "913/913 [==============================] - 3s 3ms/sample - loss: -14.1201 - acc: 0.2366 - val_loss: -8.1176 - val_acc: 0.2451\n",
      "Epoch 9/10\n",
      "913/913 [==============================] - 3s 3ms/sample - loss: -14.1240 - acc: 0.2388 - val_loss: -8.1176 - val_acc: 0.2451\n",
      "Epoch 10/10\n",
      "913/913 [==============================] - 3s 3ms/sample - loss: -14.1860 - acc: 0.2410 - val_loss: -8.1176 - val_acc: 0.2451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80b1ce3588>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_acc', \n",
    "    mode='max',\n",
    "    patience=10\n",
    ")\n",
    "# increase epochs and other steps for better training\n",
    "model.fit(seqs_mat,Y_train,batch_size=100,epochs=10,\n",
    "          validation_split=0.1,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 0s 1ms/sample - loss: -7.1038 - acc: 0.2559\n",
      "Test set\n",
      "  Loss: -7.104\n",
      "  Accuracy: 0.256\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tk.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = tf.keras.preprocessing.sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hate(user_seq):\n",
    "#     prediction\n",
    "    prob = model.predict(user_seq)\n",
    "    probability = np.mean(prob, axis=0)\n",
    "\n",
    "    if probability > 0.5:\n",
    "        return(\"Hate\")\n",
    "    elif probability < 0.5:\n",
    "        return(\"Not Hate\")\n",
    "    elif probability == 0.5:\n",
    "        return(\"Neutral\")\n",
    "\n",
    "def user_text_processing(user_text):\n",
    "    user_text = user_text.split()\n",
    "    user_text = [word.lower() for word in user_text if word not in stopwords]\n",
    "#     user_text = [lemm.lemmatize(word) for word in user_text]\n",
    "    user_text\n",
    "    user_seq = np.array(user_text)\n",
    "    user_seq = tk.texts_to_sequences(user_seq)\n",
    "    user_seq = tf.keras.preprocessing.sequence.pad_sequences(user_seq,maxlen=max_len)\n",
    "\n",
    "    return user_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 'RT @YadavsAniruddh: #श्रीनगर में एक #आतंकवादी अव पूर्व #आतंकी हो गया , #भाजपा में शामिल होते ही उसके सारे पाप धुल गए! अब वह #भाजपा के टिकट' is of 'Hate' nature\n"
     ]
    }
   ],
   "source": [
    "user_text = 'RT @YadavsAniruddh: #श्रीनगर में एक #आतंकवादी अव पूर्व #आतंकी हो गया , #भाजपा में शामिल होते ही उसके सारे पाप धुल गए! अब वह #भाजपा के टिकट'\n",
    "user_seq = user_text_processing(user_text)\n",
    "user_seq\n",
    "prediction = predict_hate(user_seq)\n",
    "print(f\"Sentence '{user_text}' is of '{prediction}' nature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
