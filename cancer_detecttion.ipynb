{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_PATH = 'archive/brain_tumor_dataset/'\n",
    "# # split the data by train/val/test\n",
    "# for CLASS in os.listdir(IMG_PATH):\n",
    "#     if not CLASS.startswith('.'):\n",
    "#         IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n",
    "#         for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n",
    "#             img = IMG_PATH + CLASS + '/' + FILE_NAME\n",
    "#             if n < 5:\n",
    "#                 shutil.copy(img, 'TEST/' + CLASS.upper() + '/' + FILE_NAME)\n",
    "#             elif n < 0.8*IMG_NUM:\n",
    "#                 shutil.copy(img, 'TRAIN/'+ CLASS.upper() + '/' + FILE_NAME)\n",
    "#             else:\n",
    "#                 shutil.copy(img, 'VAL/'+ CLASS.upper() + '/' + FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(dir_path, img_size=(100,100)):\n",
    "#     \"\"\"\n",
    "#     Load resized images as np.arrays\n",
    "#     \"\"\"\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     i = 0\n",
    "#     labels = dict()\n",
    "#     for path in sorted(os.listdir(dir_path)):\n",
    "#         if not path.startswith('.'):\n",
    "#             labels[i] = path\n",
    "#             for file in os.listdir(dir_path + path):\n",
    "#                 if not file.startswith('.'):\n",
    "#                     img = cv2.imread(dir_path + path + '/' + file)\n",
    "#                     X.append(img)\n",
    "#                     y.append(i)\n",
    "#             i += 1\n",
    "#     X = np.array(X)\n",
    "#     y = np.array(y)\n",
    "#     print(f'{len(X)} images loaded from {dir_path} directory.')\n",
    "#     return X, y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DIR = 'TRAIN/'\n",
    "# TEST_DIR = 'TEST/'\n",
    "# VAL_DIR = 'VAL/'\n",
    "# IMG_SIZE = (224,224)\n",
    "\n",
    "# X_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\n",
    "# X_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\n",
    "# X_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plotS(X, y, labels_dict, n=50):\n",
    "#     \"\"\"\n",
    "#     Creates a gridplot for desired number of images (n) from the specified set\n",
    "#     \"\"\"\n",
    "#     for ind in range(len(labels_dict)):\n",
    "#         imgs = X[np.argwhere(y == ind)][:n]\n",
    "#         j = 10\n",
    "#         i = int(n/j)\n",
    "\n",
    "#         plt.figure(figsize=(15,6))\n",
    "#         c = 1\n",
    "#         for img in imgs:\n",
    "#             plt.subplot(i,j,c)\n",
    "#             plt.imshow(img[0])\n",
    "\n",
    "#             plt.xticks([])\n",
    "#             plt.yticks([])\n",
    "#             c += 1\n",
    "#         plt.suptitle('Tumor: {}'.format(labels_dict[ind]))\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotS(X_train, y_train, labels, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from pyimagesearch https://www.pyimagesearch.com/2016/04/11/finding-extreme-points-in-contours-with-opencv/\n",
    "# def cropS(inp):\n",
    "#     new = []\n",
    "#     for img in inp:\n",
    "#         gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#         gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        \n",
    "#         thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "#         thresh = cv2.erode(thresh, None, iterations=2)\n",
    "#         thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "        \n",
    "#         cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "#         cnts = imutils.grab_contours(cnts)\n",
    "#         c = max(cnts, key=cv2.contourArea)\n",
    "        \n",
    "#         Left = tuple(c[c[:, :, 0].argmin()][0])\n",
    "#         Right = tuple(c[c[:, :, 0].argmax()][0])\n",
    "#         Top = tuple(c[c[:, :, 1].argmin()][0])\n",
    "#         Bot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "        \n",
    "#         ADD_PIXELS = 0\n",
    "#         new_img = img[Top[1]-ADD_PIXELS:Bot[1]+ADD_PIXELS, Left[0]-ADD_PIXELS:Right[0]+ADD_PIXELS].copy()\n",
    "#         new.append(new_img)\n",
    "\n",
    "#     return np.array(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_crop = cropS(X_train)\n",
    "# X_val_crop = cropS(X_val)\n",
    "# X_test_crop = cropS(X_test)\n",
    "# plotS(X_train_crop, y_train, labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_new_images(x_set, y_set, folder_name):\n",
    "#     i = 0\n",
    "#     for (img, imclass) in zip(x_set, y_set):\n",
    "#         if imclass == 0:\n",
    "#             cv2.imwrite(folder_name+'NO/'+str(i)+'.jpg', img)\n",
    "#         else:\n",
    "#             cv2.imwrite(folder_name+'YES/'+str(i)+'.jpg', img)\n",
    "#         i += 1\n",
    "# save_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP/')\n",
    "# save_new_images(X_val_crop, y_val, folder_name='VAL_CROP/')\n",
    "# save_new_images(X_test_crop, y_test, folder_name='TEST_CROP/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_imgs(inp, img_size):\n",
    "#     \"\"\"\n",
    "#     Resize and apply VGG-15 preprocessing\n",
    "#     \"\"\"\n",
    "#     new = []\n",
    "#     for img in inp:\n",
    "#         img = cv2.resize(\n",
    "#             img,\n",
    "#             dsize=img_size,\n",
    "#             interpolation=cv2.INTER_CUBIC\n",
    "#         )\n",
    "#         new.append(tf.keras.applications.vgg16.tf.keras.applications.vgg16.preprocess_input(img))\n",
    "#     return np.array(new)\n",
    "# X_train_prep = preprocess_imgs(X_train_crop, img_size=IMG_SIZE)\n",
    "# X_test_prep = preprocess_imgs(X_test_crop, img_size=IMG_SIZE)\n",
    "# X_val_prep = preprocess_imgs(X_val_crop, img_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 193 images belonging to 2 classes.\n",
      "Found 50 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN_DIR = 'TRAIN_CROP/'\n",
    "VAL_DIR = 'VAL_CROP/'\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n",
    ")\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    color_mode='rgb',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    color_mode='rgb',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.mobilenet.MobileNet(\n",
    "    input_shape=IMG_SIZE + (3,),\n",
    "    include_top=False, weights='imagenet', input_tensor=None, pooling=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition \n",
    "model = tf.keras.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/drex/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Compile \n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer = opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_acc', \n",
    "    mode='max',\n",
    "    patience=6\n",
    ")\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=50,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=25,\n",
    "    callbacks=[es]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
